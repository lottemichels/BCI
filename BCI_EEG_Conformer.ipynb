{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BCI Project: Classifying preprocessed OpenMIIR data and hvEEGNet encoded data with a Conformer model\n",
        "\n",
        "## By BrainRot: Lotte Michels & Selma Ancel\n",
        "\n",
        "This notebook was used to train the Conformer model by (Song et al., 2023) on both the pre-processed OpenMIIR data and the encoded OpenMIIR data by the hvEEGNet encoder. Please refer to our project submission for a textual description of the training steps. The notebook and comments in the code will also provide an outline of the steps that are performed.\n",
        "\n",
        "### References:\n",
        "\n",
        "* Song, Y., Zheng, Q., Liu, B. and Gao, X. (2023). EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization. IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 31, pp. 710-719, doi: 10.1109/TNSRE.2022.3230250.\n",
        "*   Stober, S. (2017). Toward Studying Music Cognition with Information Retrieval Techniques: Lessons Learned from the OpenMIIR Initiative. Frontiers in Psychology, 8. https://doi.org/10.3389/fpsyg.2017.01255. Related code is published here: https://github.com/sstober/openmiir.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QvaqBBDKw9VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmUvvaK4NYRl",
        "outputId": "5d25f6a1-92a7-4945-db6e-809882a2d8a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n",
            "Downloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_Qo1tYFWzNVi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zneI6gDDMgU8",
        "outputId": "a8d311cf-bbc4-4863-c1dd-bd9511422c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "c1_latent_features.npy\tc2_latent_features.npy\tdrive  sample_data\n",
            "/content/drive/My Drive/Colab_Notebooks/EEG_DATA\n"
          ]
        }
      ],
      "source": [
        "# Connect to drive to load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls\n",
        "%cd /content/drive/My Drive/Colab_Notebooks/EEG_DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and process the pre-processed OpenMIIR data"
      ],
      "metadata": {
        "id": "uRogSmJ33J76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "raw1 = mne.io.read_raw_fif('01_preprocessed.fif', preload=True)\n",
        "raw4 = mne.io.read_raw_fif('04_preprocessed.fif', preload=True)\n",
        "raw6 = mne.io.read_raw_fif('06_preprocessed.fif', preload=True)\n",
        "raw7 = mne.io.read_raw_fif('07_preprocessed.fif', preload=True)\n",
        "raw9 = mne.io.read_raw_fif('09_preprocessed.fif', preload=True)\n",
        "raw11 = mne.io.read_raw_fif('11_preprocessed.fif', preload=True)\n",
        "raw12 = mne.io.read_raw_fif('12_preprocessed.fif', preload=True)\n",
        "raw13 = mne.io.read_raw_fif('13_preprocessed.fif', preload=True)\n",
        "raw14 = mne.io.read_raw_fif('14_preprocessed.fif', preload=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rgnsLoeMmZa",
        "outputId": "f05279ec-4c54-4e79-948a-6c21a89f926f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening raw data file 01_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:2: RuntimeWarning: This filename (01_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw1 = mne.io.read_raw_fif('01_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 309770 =      0.000 ...  4840.156 secs\n",
            "Ready.\n",
            "Reading 0 ... 309770  =      0.000 ...  4840.156 secs...\n",
            "Opening raw data file 04_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:3: RuntimeWarning: This filename (04_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw4 = mne.io.read_raw_fif('04_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 310003 =      0.000 ...  4843.797 secs\n",
            "Ready.\n",
            "Reading 0 ... 310003  =      0.000 ...  4843.797 secs...\n",
            "Opening raw data file 06_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:4: RuntimeWarning: This filename (06_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw6 = mne.io.read_raw_fif('06_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 304128 =      0.000 ...  4752.000 secs\n",
            "Ready.\n",
            "Reading 0 ... 304128  =      0.000 ...  4752.000 secs...\n",
            "Opening raw data file 07_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:5: RuntimeWarning: This filename (07_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw7 = mne.io.read_raw_fif('07_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 315054 =      0.000 ...  4922.719 secs\n",
            "Ready.\n",
            "Reading 0 ... 315054  =      0.000 ...  4922.719 secs...\n",
            "Opening raw data file 09_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:6: RuntimeWarning: This filename (09_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw9 = mne.io.read_raw_fif('09_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 305069 =      0.000 ...  4766.703 secs\n",
            "Ready.\n",
            "Reading 0 ... 305069  =      0.000 ...  4766.703 secs...\n",
            "Opening raw data file 11_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:7: RuntimeWarning: This filename (11_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw11 = mne.io.read_raw_fif('11_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 330609 =      0.000 ...  5165.766 secs\n",
            "Ready.\n",
            "Reading 0 ... 330609  =      0.000 ...  5165.766 secs...\n",
            "Opening raw data file 12_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:8: RuntimeWarning: This filename (12_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw12 = mne.io.read_raw_fif('12_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 310668 =      0.000 ...  4854.188 secs\n",
            "Ready.\n",
            "Reading 0 ... 310668  =      0.000 ...  4854.188 secs...\n",
            "Opening raw data file 13_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:9: RuntimeWarning: This filename (13_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw13 = mne.io.read_raw_fif('13_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 313988 =      0.000 ...  4906.062 secs\n",
            "Ready.\n",
            "Reading 0 ... 313988  =      0.000 ...  4906.062 secs...\n",
            "Opening raw data file 14_preprocessed.fif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-15a9369905c1>:10: RuntimeWarning: This filename (14_preprocessed.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
            "  raw14 = mne.io.read_raw_fif('14_preprocessed.fif', preload=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Range : 0 ... 308178 =      0.000 ...  4815.281 secs\n",
            "Ready.\n",
            "Reading 0 ... 308178  =      0.000 ...  4815.281 secs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events1 = mne.find_events(raw1, stim_channel='STI 014')\n",
        "events4 = mne.find_events(raw4, stim_channel='STI 014')\n",
        "events6 = mne.find_events(raw6, stim_channel='STI 014')\n",
        "events7 = mne.find_events(raw7, stim_channel='STI 014')\n",
        "events9 = mne.find_events(raw9, stim_channel='STI 014')\n",
        "events11 = mne.find_events(raw11, stim_channel='STI 014')\n",
        "events12 = mne.find_events(raw12, stim_channel='STI 014')\n",
        "events13 = mne.find_events(raw13, stim_channel='STI 014')\n",
        "events14 = mne.find_events(raw14, stim_channel='STI 014')\n",
        "\n",
        "# List all unwanted event codes\n",
        "# For music imagery (2nd condition)\n",
        "exclude_codes1 = [1111, 2000, 2001, 11, 21, 31, 41,\n",
        "                 111, 121, 131, 141, 211, 221, 231,\n",
        "                 241, 13, 23, 33, 43, 113, 123, 133,\n",
        "                 143, 213, 223, 233, 243, 14, 24, 34,\n",
        "                 44, 114, 124, 134, 144, 214, 224, 234, 244]\n",
        "\n",
        "# For music perception (1st condition)\n",
        "exclude_codes2 = [1111, 2000, 2001, 12, 22, 32, 42, 112, 122, 132, 142,\n",
        "                 212, 222, 232, 242, 13, 23, 33, 43, 113, 123, 133,\n",
        "                 143, 213, 223, 233, 243, 14, 24, 34, 44, 114, 124,\n",
        "                 134, 144, 214, 224, 234, 244]\n",
        "\n",
        "# Keep only rows where the event code (3rd column) is NOT in exclude_codes\n",
        "filtered_events1 = events1[~np.isin(events1[:, 2], exclude_codes1)]\n",
        "filtered_events4 = events4[~np.isin(events4[:, 2], exclude_codes1)]\n",
        "filtered_events6 = events6[~np.isin(events6[:, 2], exclude_codes1)]\n",
        "filtered_events7 = events7[~np.isin(events7[:, 2], exclude_codes1)]\n",
        "filtered_events9 = events9[~np.isin(events9[:, 2], exclude_codes1)]\n",
        "filtered_events11 = events11[~np.isin(events11[:, 2], exclude_codes1)]\n",
        "filtered_events12 = events12[~np.isin(events12[:, 2], exclude_codes1)]\n",
        "filtered_events13 = events13[~np.isin(events13[:, 2], exclude_codes1)]\n",
        "filtered_events14 = events14[~np.isin(events14[:, 2], exclude_codes1)]\n",
        "\n",
        "filtered_events1_per = events1[~np.isin(events1[:, 2], exclude_codes2)]\n",
        "filtered_events4_per = events4[~np.isin(events4[:, 2], exclude_codes2)]\n",
        "filtered_events6_per = events6[~np.isin(events6[:, 2], exclude_codes2)]\n",
        "filtered_events7_per = events7[~np.isin(events7[:, 2], exclude_codes2)]\n",
        "filtered_events9_per = events9[~np.isin(events9[:, 2], exclude_codes2)]\n",
        "filtered_events11_per = events11[~np.isin(events11[:, 2], exclude_codes2)]\n",
        "filtered_events12_per = events12[~np.isin(events12[:, 2], exclude_codes2)]\n",
        "filtered_events13_per = events13[~np.isin(events13[:, 2], exclude_codes2)]\n",
        "filtered_events14_per = events14[~np.isin(events14[:, 2], exclude_codes2)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QysPSIcvRpIx",
        "outputId": "cc505911-8017-4102-e599-6b20bbee73e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2001]\n",
            "360 events found on stim channel STI 014\n",
            "Event IDs: [  11   12   13   14   21   22   23   24   31   32   33   34   41   42\n",
            "   43   44  111  112  113  114  121  122  123  124  131  132  133  134\n",
            "  141  142  143  144  211  212  213  214  221  222  223  224  231  232\n",
            "  233  234  241  242  243  244 1111 2000 2001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the events for music imagery\n",
        "\n",
        "event_id = None # any\n",
        "tmin = -0.2  # start of each epoch (200ms before the trigger) -0.2\n",
        "tmax = 6.9  # shortest song is 6.9 seconds\n",
        "detrend = 0 # remove dc\n",
        "\n",
        "#01\n",
        "picks1 = mne.pick_types(raw1.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs1 = mne.Epochs(raw1, filtered_events1, event_id, tmin, tmax, preload=True, proj=False, picks=picks1, verbose=False)\n",
        "times1 = beat_epochs1.times\n",
        "# 04\n",
        "picks4 = mne.pick_types(raw4.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs4 = mne.Epochs(raw4, filtered_events4, event_id, tmin, tmax, preload=True, proj=False, picks=picks4, verbose=False)\n",
        "times4 = beat_epochs4.times\n",
        "#06\n",
        "picks6 = mne.pick_types(raw6.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs6 = mne.Epochs(raw6, filtered_events6, event_id, tmin, tmax, preload=True, proj=False, picks=picks6, verbose=False)\n",
        "times6 = beat_epochs6.times\n",
        "#07\n",
        "picks7 = mne.pick_types(raw7.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs7 = mne.Epochs(raw7, filtered_events7, event_id, tmin, tmax, preload=True, proj=False, picks=picks7, verbose=False)\n",
        "times7 = beat_epochs7.times\n",
        "#09\n",
        "picks9 = mne.pick_types(raw9.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs9 = mne.Epochs(raw9, filtered_events9, event_id, tmin, tmax, preload=True, proj=False, picks=picks9, verbose=False)\n",
        "times9 = beat_epochs9.times\n",
        "#11\n",
        "picks11 = mne.pick_types(raw11.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs11 = mne.Epochs(raw11, filtered_events11, event_id, tmin, tmax, preload=True, proj=False, picks=picks11, verbose=False)\n",
        "times11 = beat_epochs11.times\n",
        "#12\n",
        "picks12 = mne.pick_types(raw12.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs12 = mne.Epochs(raw12, filtered_events12, event_id, tmin, tmax, preload=True, proj=False, picks=picks12, verbose=False)\n",
        "times12 = beat_epochs12.times\n",
        "#13\n",
        "picks13 = mne.pick_types(raw13.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs13 = mne.Epochs(raw13, filtered_events13, event_id, tmin, tmax, preload=True, proj=False, picks=picks13, verbose=False)\n",
        "times13 = beat_epochs13.times\n",
        "#14\n",
        "picks14 = mne.pick_types(raw14.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs14 = mne.Epochs(raw14, filtered_events14, event_id, tmin, tmax, preload=True, proj=False, picks=picks14, verbose=False)\n",
        "times14 = beat_epochs14.times"
      ],
      "metadata": {
        "id": "y3zMWPHHR97O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the events for music perception\n",
        "\n",
        "#01\n",
        "picks1 = mne.pick_types(raw1.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs1_per = mne.Epochs(raw1, filtered_events1_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks1, verbose=False)\n",
        "times1 = beat_epochs1.times\n",
        "# 04\n",
        "picks4 = mne.pick_types(raw4.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs4_per = mne.Epochs(raw4, filtered_events4_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks4, verbose=False)\n",
        "times4 = beat_epochs4.times\n",
        "#06\n",
        "picks6 = mne.pick_types(raw6.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs6_per = mne.Epochs(raw6, filtered_events6_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks6, verbose=False)\n",
        "times6 = beat_epochs6.times\n",
        "#07\n",
        "picks7 = mne.pick_types(raw7.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs7_per = mne.Epochs(raw7, filtered_events7_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks7, verbose=False)\n",
        "times7 = beat_epochs7.times\n",
        "#09\n",
        "picks9 = mne.pick_types(raw9.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs9_per = mne.Epochs(raw9, filtered_events9_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks9, verbose=False)\n",
        "times9 = beat_epochs9.times\n",
        "#11\n",
        "picks11 = mne.pick_types(raw11.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs11_per = mne.Epochs(raw11, filtered_events11_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks11, verbose=False)\n",
        "times11 = beat_epochs11.times\n",
        "#12\n",
        "picks12 = mne.pick_types(raw12.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs12_per = mne.Epochs(raw12, filtered_events12_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks12, verbose=False)\n",
        "times12 = beat_epochs12.times\n",
        "#13\n",
        "picks13 = mne.pick_types(raw13.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs13_per = mne.Epochs(raw13, filtered_events13_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks13, verbose=False)\n",
        "times13 = beat_epochs13.times\n",
        "#14\n",
        "picks14 = mne.pick_types(raw14.info, meg=False, eeg=True, eog=False, stim=False, exclude=[\"bads\"])\n",
        "beat_epochs14_per = mne.Epochs(raw14, filtered_events14_per, event_id, tmin, tmax, preload=True, proj=False, picks=picks14, verbose=False)\n",
        "times14 = beat_epochs14.times"
      ],
      "metadata": {
        "id": "kPRT7-DLyqrq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the music imagery events have been created correctly\n",
        "beat_epochs1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "1f8ap0Zb3s_L",
        "outputId": "3472dfc9-48a4-46d4-e4ae-8f321e3d9752"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Epochs | 60 events (all good), -0.203 – 6.906 s (baseline -0.203 – 0 s), ~13.4 MiB, data loaded,\n",
              " '12': 5\n",
              " '22': 5\n",
              " '32': 5\n",
              " '42': 5\n",
              " '112': 5\n",
              " '122': 5\n",
              " '132': 5\n",
              " '142': 5\n",
              " '212': 5\n",
              " '222': 5\n",
              " and 2 more events ...>"
            ],
            "text/html": [
              "<script type=\"text/javascript\">\n",
              "    // must be `var` (not `const`) because this can get embedded multiple times on a page\n",
              "var toggleVisibility = (className) => {\n",
              "\n",
              "    const elements = document.querySelectorAll(`.${className}`);\n",
              "\n",
              "    elements.forEach(element => {\n",
              "        if (element.classList.contains(\"mne-repr-section-header\")) {\n",
              "            return  // Don't collapse the section header row\n",
              "        }\n",
              "        element.classList.toggle(\"mne-repr-collapsed\");\n",
              "    });\n",
              "\n",
              "    // trigger caret to rotate\n",
              "    var sel = `.mne-repr-section-header.${className} > th.mne-repr-section-toggle > button`;\n",
              "    const button = document.querySelector(sel);\n",
              "    button.classList.toggle(\"collapsed\");\n",
              "\n",
              "    // adjust tooltip\n",
              "    sel = `tr.mne-repr-section-header.${className}`;\n",
              "    const secHeadRow = document.querySelector(sel);\n",
              "    secHeadRow.classList.toggle(\"collapsed\");\n",
              "    secHeadRow.title = secHeadRow.title === \"Hide section\" ? \"Show section\" : \"Hide section\";\n",
              "}\n",
              "</script>\n",
              "\n",
              "<style type=\"text/css\">\n",
              "    /*\n",
              "Styles in this section apply both to the sphinx-built website docs and to notebooks\n",
              "rendered in an IDE or in Jupyter. In our web docs, styles here are complemented by\n",
              "doc/_static/styles.css and other CSS files (e.g. from the sphinx theme, sphinx-gallery,\n",
              "or bootstrap). In IDEs/Jupyter, those style files are unavailable, so only the rules in\n",
              "this file apply (plus whatever default styling the IDE applies).\n",
              "*/\n",
              ".mne-repr-table {\n",
              "    display: inline;  /* prevent using full container width */\n",
              "}\n",
              ".mne-repr-table tr.mne-repr-section-header > th {\n",
              "    padding-top: 1rem;\n",
              "    text-align: left;\n",
              "    vertical-align: middle;\n",
              "}\n",
              ".mne-repr-section-toggle > button {\n",
              "    all: unset;\n",
              "    display: block;\n",
              "    height: 1rem;\n",
              "    width: 1rem;\n",
              "}\n",
              ".mne-repr-section-toggle > button > svg {\n",
              "    height: 60%;\n",
              "}\n",
              "\n",
              "/* transition (rotation) effects on the collapser button */\n",
              ".mne-repr-section-toggle > button.collapsed > svg {\n",
              "    transition: 0.1s ease-out;\n",
              "    transform: rotate(-90deg);\n",
              "}\n",
              ".mne-repr-section-toggle > button:not(.collapsed) > svg {\n",
              "    transition: 0.1s ease-out;\n",
              "    transform: rotate(0deg);\n",
              "}\n",
              "\n",
              "/* hide collapsed table rows */\n",
              ".mne-repr-collapsed {\n",
              "    display: none;\n",
              "}\n",
              "\n",
              "\n",
              "@layer {\n",
              "    /*\n",
              "    Selectors in a `@layer` will always be lower-precedence than selectors outside the\n",
              "    layer. So even though e.g. `div.output_html` is present in the sphinx-rendered\n",
              "    website docs, the styles here won't take effect there as long as some other rule\n",
              "    somewhere in the page's CSS targets the same element.\n",
              "\n",
              "    In IDEs or Jupyter notebooks, though, the CSS files from the sphinx theme,\n",
              "    sphinx-gallery, and bootstrap are unavailable, so these styles will apply.\n",
              "\n",
              "    Notes:\n",
              "\n",
              "    - the selector `.accordion-body` is for MNE Reports\n",
              "    - the selector `.output_html` is for VSCode's notebook interface\n",
              "    - the selector `.jp-RenderedHTML` is for Jupyter notebook\n",
              "    - variables starting with `--theme-` are VSCode-specific.\n",
              "    - variables starting with `--jp-` are Jupyter styles, *some of which* are also\n",
              "      available in VSCode. Here we try the `--theme-` variable first, then fall back to\n",
              "      the `--jp-` ones.\n",
              "    */\n",
              "    .mne-repr-table {\n",
              "        --mne-toggle-color: var(--theme-foreground, var(--jp-ui-font-color1));\n",
              "        --mne-button-bg-color: var(--theme-button-background, var(--jp-info-color0, var(--jp-content-link-color)));\n",
              "        --mne-button-fg-color: var(--theme-button-foreground, var(--jp-ui-inverse-font-color0, var(--jp-editor-background)));\n",
              "        --mne-button-hover-bg-color: var(--theme-button-hover-background, var(--jp-info-color1));\n",
              "        --mne-button-radius: var(--jp-border-radius, 0.25rem);\n",
              "    }\n",
              "    /* chevron position/alignment; in VSCode it looks ok without adjusting */\n",
              "    .accordion-body .mne-repr-section-toggle > button,\n",
              "    .jp-RenderedHTML .mne-repr-section-toggle > button {\n",
              "        padding: 0 0 45% 25% !important;\n",
              "    }\n",
              "    /* chevron color; MNE Report doesn't have light/dark mode */\n",
              "    div.output_html .mne-repr-section-toggle > button > svg > path,\n",
              "    .jp-RenderedHTML .mne-repr-section-toggle > button > svg > path {\n",
              "        fill: var(--mne-toggle-color);\n",
              "    }\n",
              "    .accordion-body .mne-ch-names-btn,\n",
              "    div.output_html .mne-ch-names-btn,\n",
              "    .jp-RenderedHTML .mne-ch-names-btn {\n",
              "        -webkit-border-radius: var(--mne-button-radius);\n",
              "        -moz-border-radius: var(--mne-button-radius);\n",
              "        border-radius: var(--mne-button-radius);\n",
              "        border: none;\n",
              "        background-image: none;\n",
              "        background-color: var(--mne-button-bg-color);\n",
              "        color: var(--mne-button-fg-color);\n",
              "        font-size: inherit;\n",
              "        min-width: 1.5rem;\n",
              "        padding: 0.25rem;\n",
              "        text-align: center;\n",
              "        text-decoration: none;\n",
              "    }\n",
              "    .accordion-body .mne-ch-names-btn:hover,\n",
              "    div.output_html .mne.ch-names-btn:hover,\n",
              "    .jp-RenderedHTML .mne-ch-names-btn:hover {\n",
              "        background-color: var(--mne-button-hover-bg-color);\n",
              "        text-decoration: underline;\n",
              "    }\n",
              "    .accordion-body .mne-ch-names-btn:focus-visible,\n",
              "    div.output_html .mne-ch-names-btn:focus-visible,\n",
              "    .jp-RenderedHTML .mne-ch-names-btn:focus-visible {\n",
              "        outline: 0.1875rem solid var(--mne-button-bg-color) !important;\n",
              "        outline-offset: 0.1875rem !important;\n",
              "    }\n",
              "}\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "<table class=\"table mne-repr-table\">\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header general-28b43ec0-2f7a-4d5f-a136-aeee3dabff94\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('general-28b43ec0-2f7a-4d5f-a136-aeee3dabff94')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>General</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element general-28b43ec0-2f7a-4d5f-a136-aeee3dabff94 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>MNE object type</td>\n",
              "    <td>Epochs</td>\n",
              "</tr>\n",
              "<tr class=\"repr-element general-28b43ec0-2f7a-4d5f-a136-aeee3dabff94 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Measurement date</td>\n",
              "    \n",
              "    <td>2015-01-28 at 17:39:57 UTC</td>\n",
              "    \n",
              "</tr>\n",
              "<tr class=\"repr-element general-28b43ec0-2f7a-4d5f-a136-aeee3dabff94 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Participant</td>\n",
              "    \n",
              "    <td>Unknown</td>\n",
              "    \n",
              "</tr>\n",
              "<tr class=\"repr-element general-28b43ec0-2f7a-4d5f-a136-aeee3dabff94 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Experimenter</td>\n",
              "    \n",
              "    <td>Unknown</td>\n",
              "    \n",
              "</tr>\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>Acquisition</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Total number of events</td>\n",
              "    <td>60</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Events counts</td>\n",
              "    \n",
              "    <td>\n",
              "        \n",
              "        112: 5\n",
              "        <br />\n",
              "        \n",
              "        12: 5\n",
              "        <br />\n",
              "        \n",
              "        122: 5\n",
              "        <br />\n",
              "        \n",
              "        132: 5\n",
              "        <br />\n",
              "        \n",
              "        142: 5\n",
              "        <br />\n",
              "        \n",
              "        212: 5\n",
              "        <br />\n",
              "        \n",
              "        22: 5\n",
              "        <br />\n",
              "        \n",
              "        222: 5\n",
              "        <br />\n",
              "        \n",
              "        232: 5\n",
              "        <br />\n",
              "        \n",
              "        242: 5\n",
              "        <br />\n",
              "        \n",
              "        32: 5\n",
              "        <br />\n",
              "        \n",
              "        42: 5\n",
              "        \n",
              "        \n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Time range</td>\n",
              "    <td>-0.203 – 6.906 s</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Baseline</td>\n",
              "    <td>-0.203 – 0.000 s</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Sampling frequency</td>\n",
              "    <td>64.00 Hz</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Time points</td>\n",
              "    <td>456</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-75c703a5-6cfc-4c35-9b5f-3ca2a16e84b0 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Metadata</td>\n",
              "    <td>No metadata set</td>\n",
              "</tr>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header channels-eacdff1a-2dfc-4ec1-9f61-ed452a463935\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('channels-eacdff1a-2dfc-4ec1-9f61-ed452a463935')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>Channels</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "    \n",
              "<tr class=\"repr-element channels-eacdff1a-2dfc-4ec1-9f61-ed452a463935 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>EEG</td>\n",
              "    <td>\n",
              "        <button class=\"mne-ch-names-btn sd-sphinx-override sd-btn sd-btn-info sd-text-wrap sd-shadow-sm\" onclick=\"alert('Good EEG:\\n\\nFp1, AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, TP7, CP5, CP3, CP1, P1, P3, P5, P7, P9, PO7, PO3, O1, Iz, Oz, POz, Pz, CPz, Fpz, Fp2, AF8, AF4, AFz, Fz, F2, F4, F6, F8, FT8, FC6, FC4, FC2, FCz, Cz, C2, C4, C6, T8, TP8, CP6, CP4, CP2, P2, P4, P6, P8, P10, PO8, PO4, O2')\" title=\"(Click to open in popup)&#13;&#13;Fp1, AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, TP7, CP5, CP3, CP1, P1, P3, P5, P7, P9, PO7, PO3, O1, Iz, Oz, POz, Pz, CPz, Fpz, Fp2, AF8, AF4, AFz, Fz, F2, F4, F6, F8, FT8, FC6, FC4, FC2, FCz, Cz, C2, C4, C6, T8, TP8, CP6, CP4, CP2, P2, P4, P6, P8, P10, PO8, PO4, O2\">\n",
              "            64\n",
              "        </button>\n",
              "\n",
              "        \n",
              "    </td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element channels-eacdff1a-2dfc-4ec1-9f61-ed452a463935 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Head & sensor digitization</td>\n",
              "    \n",
              "    <td>67 points</td>\n",
              "    \n",
              "</tr>\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header filters-2b435885-0d8e-4f5b-8aca-01c1eec5ec11\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('filters-2b435885-0d8e-4f5b-8aca-01c1eec5ec11')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>Filters</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element filters-2b435885-0d8e-4f5b-8aca-01c1eec5ec11 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Highpass</td>\n",
              "    <td>0.50 Hz</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element filters-2b435885-0d8e-4f5b-8aca-01c1eec5ec11 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Lowpass</td>\n",
              "    <td>30.00 Hz</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the music perception events have been created correctly\n",
        "beat_epochs1_per"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "DiTAKrd-SzpL",
        "outputId": "097a39a0-48d9-4a51-95dd-a8c0c44db534"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Epochs | 60 events (all good), -0.203 – 6.906 s (baseline -0.203 – 0 s), ~13.4 MiB, data loaded,\n",
              " '11': 5\n",
              " '21': 5\n",
              " '31': 5\n",
              " '41': 5\n",
              " '111': 5\n",
              " '121': 5\n",
              " '131': 5\n",
              " '141': 5\n",
              " '211': 5\n",
              " '221': 5\n",
              " and 2 more events ...>"
            ],
            "text/html": [
              "<script type=\"text/javascript\">\n",
              "    // must be `var` (not `const`) because this can get embedded multiple times on a page\n",
              "var toggleVisibility = (className) => {\n",
              "\n",
              "    const elements = document.querySelectorAll(`.${className}`);\n",
              "\n",
              "    elements.forEach(element => {\n",
              "        if (element.classList.contains(\"mne-repr-section-header\")) {\n",
              "            return  // Don't collapse the section header row\n",
              "        }\n",
              "        element.classList.toggle(\"mne-repr-collapsed\");\n",
              "    });\n",
              "\n",
              "    // trigger caret to rotate\n",
              "    var sel = `.mne-repr-section-header.${className} > th.mne-repr-section-toggle > button`;\n",
              "    const button = document.querySelector(sel);\n",
              "    button.classList.toggle(\"collapsed\");\n",
              "\n",
              "    // adjust tooltip\n",
              "    sel = `tr.mne-repr-section-header.${className}`;\n",
              "    const secHeadRow = document.querySelector(sel);\n",
              "    secHeadRow.classList.toggle(\"collapsed\");\n",
              "    secHeadRow.title = secHeadRow.title === \"Hide section\" ? \"Show section\" : \"Hide section\";\n",
              "}\n",
              "</script>\n",
              "\n",
              "<style type=\"text/css\">\n",
              "    /*\n",
              "Styles in this section apply both to the sphinx-built website docs and to notebooks\n",
              "rendered in an IDE or in Jupyter. In our web docs, styles here are complemented by\n",
              "doc/_static/styles.css and other CSS files (e.g. from the sphinx theme, sphinx-gallery,\n",
              "or bootstrap). In IDEs/Jupyter, those style files are unavailable, so only the rules in\n",
              "this file apply (plus whatever default styling the IDE applies).\n",
              "*/\n",
              ".mne-repr-table {\n",
              "    display: inline;  /* prevent using full container width */\n",
              "}\n",
              ".mne-repr-table tr.mne-repr-section-header > th {\n",
              "    padding-top: 1rem;\n",
              "    text-align: left;\n",
              "    vertical-align: middle;\n",
              "}\n",
              ".mne-repr-section-toggle > button {\n",
              "    all: unset;\n",
              "    display: block;\n",
              "    height: 1rem;\n",
              "    width: 1rem;\n",
              "}\n",
              ".mne-repr-section-toggle > button > svg {\n",
              "    height: 60%;\n",
              "}\n",
              "\n",
              "/* transition (rotation) effects on the collapser button */\n",
              ".mne-repr-section-toggle > button.collapsed > svg {\n",
              "    transition: 0.1s ease-out;\n",
              "    transform: rotate(-90deg);\n",
              "}\n",
              ".mne-repr-section-toggle > button:not(.collapsed) > svg {\n",
              "    transition: 0.1s ease-out;\n",
              "    transform: rotate(0deg);\n",
              "}\n",
              "\n",
              "/* hide collapsed table rows */\n",
              ".mne-repr-collapsed {\n",
              "    display: none;\n",
              "}\n",
              "\n",
              "\n",
              "@layer {\n",
              "    /*\n",
              "    Selectors in a `@layer` will always be lower-precedence than selectors outside the\n",
              "    layer. So even though e.g. `div.output_html` is present in the sphinx-rendered\n",
              "    website docs, the styles here won't take effect there as long as some other rule\n",
              "    somewhere in the page's CSS targets the same element.\n",
              "\n",
              "    In IDEs or Jupyter notebooks, though, the CSS files from the sphinx theme,\n",
              "    sphinx-gallery, and bootstrap are unavailable, so these styles will apply.\n",
              "\n",
              "    Notes:\n",
              "\n",
              "    - the selector `.accordion-body` is for MNE Reports\n",
              "    - the selector `.output_html` is for VSCode's notebook interface\n",
              "    - the selector `.jp-RenderedHTML` is for Jupyter notebook\n",
              "    - variables starting with `--theme-` are VSCode-specific.\n",
              "    - variables starting with `--jp-` are Jupyter styles, *some of which* are also\n",
              "      available in VSCode. Here we try the `--theme-` variable first, then fall back to\n",
              "      the `--jp-` ones.\n",
              "    */\n",
              "    .mne-repr-table {\n",
              "        --mne-toggle-color: var(--theme-foreground, var(--jp-ui-font-color1));\n",
              "        --mne-button-bg-color: var(--theme-button-background, var(--jp-info-color0, var(--jp-content-link-color)));\n",
              "        --mne-button-fg-color: var(--theme-button-foreground, var(--jp-ui-inverse-font-color0, var(--jp-editor-background)));\n",
              "        --mne-button-hover-bg-color: var(--theme-button-hover-background, var(--jp-info-color1));\n",
              "        --mne-button-radius: var(--jp-border-radius, 0.25rem);\n",
              "    }\n",
              "    /* chevron position/alignment; in VSCode it looks ok without adjusting */\n",
              "    .accordion-body .mne-repr-section-toggle > button,\n",
              "    .jp-RenderedHTML .mne-repr-section-toggle > button {\n",
              "        padding: 0 0 45% 25% !important;\n",
              "    }\n",
              "    /* chevron color; MNE Report doesn't have light/dark mode */\n",
              "    div.output_html .mne-repr-section-toggle > button > svg > path,\n",
              "    .jp-RenderedHTML .mne-repr-section-toggle > button > svg > path {\n",
              "        fill: var(--mne-toggle-color);\n",
              "    }\n",
              "    .accordion-body .mne-ch-names-btn,\n",
              "    div.output_html .mne-ch-names-btn,\n",
              "    .jp-RenderedHTML .mne-ch-names-btn {\n",
              "        -webkit-border-radius: var(--mne-button-radius);\n",
              "        -moz-border-radius: var(--mne-button-radius);\n",
              "        border-radius: var(--mne-button-radius);\n",
              "        border: none;\n",
              "        background-image: none;\n",
              "        background-color: var(--mne-button-bg-color);\n",
              "        color: var(--mne-button-fg-color);\n",
              "        font-size: inherit;\n",
              "        min-width: 1.5rem;\n",
              "        padding: 0.25rem;\n",
              "        text-align: center;\n",
              "        text-decoration: none;\n",
              "    }\n",
              "    .accordion-body .mne-ch-names-btn:hover,\n",
              "    div.output_html .mne.ch-names-btn:hover,\n",
              "    .jp-RenderedHTML .mne-ch-names-btn:hover {\n",
              "        background-color: var(--mne-button-hover-bg-color);\n",
              "        text-decoration: underline;\n",
              "    }\n",
              "    .accordion-body .mne-ch-names-btn:focus-visible,\n",
              "    div.output_html .mne-ch-names-btn:focus-visible,\n",
              "    .jp-RenderedHTML .mne-ch-names-btn:focus-visible {\n",
              "        outline: 0.1875rem solid var(--mne-button-bg-color) !important;\n",
              "        outline-offset: 0.1875rem !important;\n",
              "    }\n",
              "}\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "<table class=\"table mne-repr-table\">\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header general-321497d4-00f4-4fb2-a999-55f4268a26ab\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('general-321497d4-00f4-4fb2-a999-55f4268a26ab')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>General</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element general-321497d4-00f4-4fb2-a999-55f4268a26ab \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>MNE object type</td>\n",
              "    <td>Epochs</td>\n",
              "</tr>\n",
              "<tr class=\"repr-element general-321497d4-00f4-4fb2-a999-55f4268a26ab \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Measurement date</td>\n",
              "    \n",
              "    <td>2015-01-28 at 17:39:57 UTC</td>\n",
              "    \n",
              "</tr>\n",
              "<tr class=\"repr-element general-321497d4-00f4-4fb2-a999-55f4268a26ab \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Participant</td>\n",
              "    \n",
              "    <td>Unknown</td>\n",
              "    \n",
              "</tr>\n",
              "<tr class=\"repr-element general-321497d4-00f4-4fb2-a999-55f4268a26ab \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Experimenter</td>\n",
              "    \n",
              "    <td>Unknown</td>\n",
              "    \n",
              "</tr>\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header acquisition-c5f231e6-7c16-4032-a541-4b7285307b88\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('acquisition-c5f231e6-7c16-4032-a541-4b7285307b88')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>Acquisition</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Total number of events</td>\n",
              "    <td>60</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Events counts</td>\n",
              "    \n",
              "    <td>\n",
              "        \n",
              "        11: 5\n",
              "        <br />\n",
              "        \n",
              "        111: 5\n",
              "        <br />\n",
              "        \n",
              "        121: 5\n",
              "        <br />\n",
              "        \n",
              "        131: 5\n",
              "        <br />\n",
              "        \n",
              "        141: 5\n",
              "        <br />\n",
              "        \n",
              "        21: 5\n",
              "        <br />\n",
              "        \n",
              "        211: 5\n",
              "        <br />\n",
              "        \n",
              "        221: 5\n",
              "        <br />\n",
              "        \n",
              "        231: 5\n",
              "        <br />\n",
              "        \n",
              "        241: 5\n",
              "        <br />\n",
              "        \n",
              "        31: 5\n",
              "        <br />\n",
              "        \n",
              "        41: 5\n",
              "        \n",
              "        \n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Time range</td>\n",
              "    <td>-0.203 – 6.906 s</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Baseline</td>\n",
              "    <td>-0.203 – 0.000 s</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Sampling frequency</td>\n",
              "    <td>64.00 Hz</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Time points</td>\n",
              "    <td>456</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element acquisition-c5f231e6-7c16-4032-a541-4b7285307b88 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Metadata</td>\n",
              "    <td>No metadata set</td>\n",
              "</tr>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header channels-6d8d3006-b1a9-4343-8046-22b9e9b8f2a1\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('channels-6d8d3006-b1a9-4343-8046-22b9e9b8f2a1')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>Channels</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "    \n",
              "<tr class=\"repr-element channels-6d8d3006-b1a9-4343-8046-22b9e9b8f2a1 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>EEG</td>\n",
              "    <td>\n",
              "        <button class=\"mne-ch-names-btn sd-sphinx-override sd-btn sd-btn-info sd-text-wrap sd-shadow-sm\" onclick=\"alert('Good EEG:\\n\\nFp1, AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, TP7, CP5, CP3, CP1, P1, P3, P5, P7, P9, PO7, PO3, O1, Iz, Oz, POz, Pz, CPz, Fpz, Fp2, AF8, AF4, AFz, Fz, F2, F4, F6, F8, FT8, FC6, FC4, FC2, FCz, Cz, C2, C4, C6, T8, TP8, CP6, CP4, CP2, P2, P4, P6, P8, P10, PO8, PO4, O2')\" title=\"(Click to open in popup)&#13;&#13;Fp1, AF7, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, TP7, CP5, CP3, CP1, P1, P3, P5, P7, P9, PO7, PO3, O1, Iz, Oz, POz, Pz, CPz, Fpz, Fp2, AF8, AF4, AFz, Fz, F2, F4, F6, F8, FT8, FC6, FC4, FC2, FCz, Cz, C2, C4, C6, T8, TP8, CP6, CP4, CP2, P2, P4, P6, P8, P10, PO8, PO4, O2\">\n",
              "            64\n",
              "        </button>\n",
              "\n",
              "        \n",
              "    </td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element channels-6d8d3006-b1a9-4343-8046-22b9e9b8f2a1 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Head & sensor digitization</td>\n",
              "    \n",
              "    <td>67 points</td>\n",
              "    \n",
              "</tr>\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "<tr class=\"mne-repr-section-header filters-3f1c00ba-bb17-4efd-9f98-8883165a4df1\"\n",
              "     title=\"Hide section\" \n",
              "    onclick=\"toggleVisibility('filters-3f1c00ba-bb17-4efd-9f98-8883165a4df1')\">\n",
              "    <th class=\"mne-repr-section-toggle\">\n",
              "        <button >\n",
              "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
              "        </button>\n",
              "    </th>\n",
              "    <th colspan=\"2\">\n",
              "        <strong>Filters</strong>\n",
              "    </th>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element filters-3f1c00ba-bb17-4efd-9f98-8883165a4df1 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Highpass</td>\n",
              "    <td>0.50 Hz</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "<tr class=\"repr-element filters-3f1c00ba-bb17-4efd-9f98-8883165a4df1 \">\n",
              "    <td class=\"mne-repr-section-toggle\"></td>\n",
              "    <td>Lowpass</td>\n",
              "    <td>30.00 Hz</td>\n",
              "</tr>\n",
              "\n",
              "\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add all the X and y data for music imagery together\n",
        "X1 = beat_epochs1.get_data()\n",
        "y1 = beat_epochs1.events[:, 2]\n",
        "X4 = beat_epochs4.get_data()\n",
        "y4 = beat_epochs4.events[:, 2]\n",
        "X6 = beat_epochs6.get_data()\n",
        "y6 = beat_epochs6.events[:, 2]\n",
        "X7 = beat_epochs7.get_data()\n",
        "y7 = beat_epochs7.events[:, 2]\n",
        "X9 = beat_epochs9.get_data()\n",
        "y9 = beat_epochs9.events[:, 2]\n",
        "X11 = beat_epochs11.get_data()\n",
        "y11 = beat_epochs11.events[:, 2]\n",
        "X12 = beat_epochs12.get_data()\n",
        "y12 = beat_epochs12.events[:, 2]\n",
        "X13 = beat_epochs13.get_data()\n",
        "y13 = beat_epochs13.events[:, 2]\n",
        "X14 = beat_epochs14.get_data()\n",
        "y14 = beat_epochs14.events[:, 2]\n",
        "\n",
        "X = np.concatenate((X1, X4, X6, X7, X9, X11, X12, X13, X14), axis=0)\n",
        "y = np.concatenate((y1, y4, y6, y7, y9, y11, y12, y13, y14), axis=0)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "LeCKQycLTB2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7cedb0-1c78-4c1d-d505-3c31322ecf30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(540, 64, 456)\n",
            "(540,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add all the X and y data for music perception together\n",
        "X1_per = beat_epochs1_per.get_data()\n",
        "y1_per = beat_epochs1_per.events[:, 2]\n",
        "X4_per = beat_epochs4_per.get_data()\n",
        "y4_per = beat_epochs4_per.events[:, 2]\n",
        "X6_per = beat_epochs6_per.get_data()\n",
        "y6_per = beat_epochs6_per.events[:, 2]\n",
        "X7_per = beat_epochs7_per.get_data()\n",
        "y7_per = beat_epochs7_per.events[:, 2]\n",
        "X9_per = beat_epochs9_per.get_data()\n",
        "y9_per = beat_epochs9_per.events[:, 2]\n",
        "X11_per = beat_epochs11_per.get_data()\n",
        "y11_per = beat_epochs11_per.events[:, 2]\n",
        "X12_per = beat_epochs12_per.get_data()\n",
        "y12_per = beat_epochs12_per.events[:, 2]\n",
        "X13_per = beat_epochs13_per.get_data()\n",
        "y13_per = beat_epochs13_per.events[:, 2]\n",
        "X14_per = beat_epochs14_per.get_data()\n",
        "y14_per = beat_epochs14_per.events[:, 2]\n",
        "\n",
        "X_per = np.concatenate((X1_per, X4_per, X6_per, X7_per, X9_per, X11_per, X12_per, X13_per, X14_per), axis=0)\n",
        "y_per = np.concatenate((y1_per, y4_per, y6_per, y7_per, y9_per, y11_per, y12_per, y13_per, y14_per), axis=0)\n",
        "\n",
        "print(X_per.shape)\n",
        "print(y_per.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOgLsNMwzMDS",
        "outputId": "457c26b3-fed2-41db-d4d9-24f1aea96ec3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(540, 64, 456)\n",
            "(540,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize all trial channels to zero mean and range [−1, 1].\n",
        "mean = np.mean(X, axis=2, keepdims=True)\n",
        "zero_mean_data = X - mean\n",
        "\n",
        "min_val = np.min(zero_mean_data, axis=2, keepdims=True)\n",
        "max_val = np.max(zero_mean_data, axis=2, keepdims=True)\n",
        "\n",
        "range_val = max_val - min_val\n",
        "range_val[range_val == 0] = 1\n",
        "\n",
        "norm_X = 2 * (zero_mean_data - min_val) / range_val - 1"
      ],
      "metadata": {
        "id": "xcwmsxAPvTHk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize all trial channels to zero mean and range [−1, 1].\n",
        "mean = np.mean(X_per, axis=2, keepdims=True)\n",
        "zero_mean_data = X_per - mean\n",
        "\n",
        "min_val = np.min(zero_mean_data, axis=2, keepdims=True)\n",
        "max_val = np.max(zero_mean_data, axis=2, keepdims=True)\n",
        "\n",
        "range_val = max_val - min_val\n",
        "range_val[range_val == 0] = 1\n",
        "\n",
        "norm_X_per = 2 * (zero_mean_data - min_val) / range_val - 1"
      ],
      "metadata": {
        "id": "L76pc6oIz0-G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split the data\n",
        "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(norm_X, y_encoded, test_size=0.2, random_state=42, stratify = y_encoded)"
      ],
      "metadata": {
        "id": "mBDP9pMXKtzj"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded_per = le.fit_transform(y_per)\n",
        "\n",
        "# Split the data\n",
        "X_train_per, X_test_per, y_train_per, y_test_per = train_test_split(norm_X_per, y_encoded_per, test_size=0.2, random_state=42, stratify = y_encoded_per)"
      ],
      "metadata": {
        "id": "RyZAXGqh0ADg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels for the group condition\n",
        "group_y_train_img = []\n",
        "group_y_test_img = []\n",
        "for i in y_train_img:\n",
        "    if i < 4:\n",
        "        group_y_train_img.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_train_img.append(1)\n",
        "    else:\n",
        "        group_y_train_img.append(2)\n",
        "\n",
        "for i in y_test_img:\n",
        "    if i < 4:\n",
        "        group_y_test_img.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_test_img.append(1)\n",
        "    else:\n",
        "        group_y_test_img.append(2)\n",
        "\n",
        "group_y_train_per = []\n",
        "group_y_test_per = []\n",
        "for i in y_train_per:\n",
        "    if i < 4:\n",
        "        group_y_train_per.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_train_per.append(1)\n",
        "    else:\n",
        "        group_y_train_per.append(2)\n",
        "\n",
        "for i in y_test_per:\n",
        "    if i < 4:\n",
        "        group_y_test_per.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_test_per.append(1)\n",
        "    else:\n",
        "        group_y_test_per.append(2)"
      ],
      "metadata": {
        "id": "Y6XbNRyn6Axn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels for the meter condition\n",
        "meter_y_train_img = []\n",
        "meter_y_test_img = []\n",
        "for i in y_train_img:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_train_img.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_train_img.append(1)\n",
        "\n",
        "for i in y_test_img:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_test_img.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_test_img.append(1)\n",
        "\n",
        "meter_y_train_per = []\n",
        "meter_y_test_per = []\n",
        "for i in y_train_per:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_train_per.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_train_per.append(1)\n",
        "\n",
        "for i in y_test_per:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_test_per.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_test_per.append(1)"
      ],
      "metadata": {
        "id": "aPvHi15iBdiU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conformer model"
      ],
      "metadata": {
        "id": "us6EtEkSId_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EEG Conformer\n",
        "\n",
        "Convolutional Transformer for EEG decoding\n",
        "\n",
        "Couple CNN and Transformer in a concise manner with amazing results\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "gpus = [0]\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import itertools\n",
        "import datetime\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "import scipy.io\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import torch.autograd as autograd\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "# from common_spatial_pattern import csp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "\n",
        "# Convolution module for\n",
        "# use conv to capture local features, instead of postion embedding.\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, num_channels, emb_size=40):\n",
        "        # self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "\n",
        "        self.shallownet = nn.Sequential(\n",
        "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
        "            nn.Conv2d(40, 40, (num_channels, 1), (1, 1)),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.shallownet(x)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 num_heads=10,\n",
        "                 drop_p=0.5,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size, n_classes, size):\n",
        "        super().__init__()\n",
        "\n",
        "        # global average pooling\n",
        "        self.clshead = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        out = self.fc(x)\n",
        "        return x, out\n",
        "\n",
        "class Conformer(nn.Sequential):\n",
        "    def __init__(self, num_classes, num_channels, size, emb_size=40, depth=6):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(emb_size=emb_size, num_channels=num_channels),\n",
        "            TransformerEncoder(depth=depth, emb_size=emb_size),\n",
        "            ClassificationHead(emb_size, num_classes, size)\n",
        "        )\n",
        "\n",
        "\n",
        "class ExP():\n",
        "    def __init__(self, nsub, nclasses, nchannels, size):\n",
        "        super(ExP, self).__init__()\n",
        "        self.batch_size = 72\n",
        "        self.n_epochs = 2000\n",
        "        self.c_dim = 12\n",
        "        self.lr = 0.0002\n",
        "        self.b1 = 0.5\n",
        "        self.b2 = 0.999\n",
        "        self.dimension = (190, 50)\n",
        "        self.nSub = nsub\n",
        "        self.num_classes = nclasses\n",
        "        self.num_channels = nchannels\n",
        "        self.size = size\n",
        "\n",
        "        self.start_epoch = 0\n",
        "\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor\n",
        "        self.LongTensor = torch.cuda.LongTensor\n",
        "\n",
        "        self.criterion_l1 = torch.nn.L1Loss().cuda()\n",
        "        self.criterion_l2 = torch.nn.MSELoss().cuda()\n",
        "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        self.model = Conformer(self.num_classes, self.num_channels, self.size).cuda()\n",
        "        self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
        "        self.model = self.model.cuda()\n",
        "        # summary(self.model, (1, 64, 456))\n",
        "\n",
        "    def get_source_data(self, X_train, y_train, X_test, y_test, enc):\n",
        "        # ! please please recheck if you need validation set\n",
        "        # ! and the data segement compared methods used\n",
        "\n",
        "        # train data\n",
        "        self.train_data = X_train\n",
        "        self.train_label = y_train\n",
        "\n",
        "        if enc == False:\n",
        "          self.train_data = np.expand_dims(self.train_data, axis=1)\n",
        "        self.train_label = np.transpose(self.train_label)\n",
        "\n",
        "        self.allData = self.train_data\n",
        "        self.allLabel = self.train_label\n",
        "\n",
        "        shuffle_num = np.random.permutation(len(self.allData))\n",
        "        self.allData = self.allData[shuffle_num, :, :, :]\n",
        "        self.allLabel = self.allLabel[shuffle_num]\n",
        "\n",
        "        # test data\n",
        "        self.test_data = X_test\n",
        "        self.test_label = y_test\n",
        "\n",
        "        if enc == False:\n",
        "          self.test_data = np.expand_dims(self.test_data, axis=1)\n",
        "        self.test_label = np.transpose(self.test_label)\n",
        "\n",
        "        self.testData = self.test_data\n",
        "        self.testLabel = self.test_label\n",
        "\n",
        "        print(self.allData.shape)\n",
        "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, X_test, y_test, enc):\n",
        "\n",
        "        img, label, test_data, test_label = self.get_source_data(X_train, y_train, X_test, y_test, enc)\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        label = torch.from_numpy(label)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(img, label)\n",
        "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        test_data = torch.from_numpy(test_data)\n",
        "        test_label = torch.from_numpy(test_label)\n",
        "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "\n",
        "        test_data = Variable(test_data.type(self.Tensor))\n",
        "        test_label = Variable(test_label.type(self.LongTensor))\n",
        "\n",
        "        bestAcc = 0\n",
        "        averAcc = 0\n",
        "        num = 0\n",
        "        Y_true = 0\n",
        "        Y_pred = 0\n",
        "\n",
        "        # Train the cnn model\n",
        "        total_step = len(self.dataloader)\n",
        "        curr_lr = self.lr\n",
        "\n",
        "        for e in range(self.n_epochs):\n",
        "            # in_epoch = time.time()\n",
        "            self.model.train()\n",
        "            for i, (img, label) in enumerate(self.dataloader):\n",
        "\n",
        "                img = Variable(img.cuda().type(self.Tensor))\n",
        "                label = Variable(label.cuda().type(self.LongTensor))\n",
        "\n",
        "                tok, outputs = self.model(img)\n",
        "\n",
        "                loss = self.criterion_cls(outputs, label)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "\n",
        "            # out_epoch = time.time()\n",
        "\n",
        "            # test process\n",
        "            if (e + 1) % 1 == 0:\n",
        "                self.model.eval()\n",
        "                Tok, Cls = self.model(test_data)\n",
        "\n",
        "\n",
        "                loss_test = self.criterion_cls(Cls, test_label)\n",
        "                y_pred = torch.max(Cls, 1)[1]\n",
        "                acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
        "                train_pred = torch.max(outputs, 1)[1]\n",
        "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
        "\n",
        "                if e == 1999:\n",
        "                  print('Epoch:', e + 1,\n",
        "                        '  Train loss: %.6f' % loss.detach().cpu().numpy(),\n",
        "                        '  Test loss: %.6f' % loss_test.detach().cpu().numpy(),\n",
        "                        '  Train accuracy %.6f' % train_acc,\n",
        "                        '  Test accuracy is %.6f' % acc)\n",
        "\n",
        "                num = num + 1\n",
        "                averAcc = averAcc + acc\n",
        "                if acc > bestAcc:\n",
        "                    bestAcc = acc\n",
        "                    Y_true = test_label\n",
        "                    Y_pred = y_pred\n",
        "\n",
        "\n",
        "        averAcc = averAcc / num\n",
        "        #print('The average accuracy is:', averAcc)\n",
        "        #print('The best accuracy is:', bestAcc)\n",
        "\n",
        "        return bestAcc, averAcc, Y_true, Y_pred\n",
        "        # writer.close()"
      ],
      "metadata": {
        "id": "b2Z0-_mO-vzV"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of the OpenMIIR data"
      ],
      "metadata": {
        "id": "Ze3JlS4I-tqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stimulus (12) Classification"
      ],
      "metadata": {
        "id": "5Hbl0ZL1wvky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Imagery"
      ],
      "metadata": {
        "id": "swIYNI9dHAlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Imagery (stimulus)')\n",
        "exp = ExP(1, 12, 64, 960)\n",
        "\n",
        "enc = False\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_img, y_train_img, X_test_img, y_test_img, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TVucZxDikwc",
        "outputId": "22c5247b-f198-4d29-a6f1-c3f20555936e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Imagery (stimulus)\n",
            "(432, 1, 64, 456)\n",
            "Epoch: 2000   Train loss: 0.007763   Test loss: 19.196632   Train accuracy 1.000000   Test accuracy is 0.064815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Perception"
      ],
      "metadata": {
        "id": "sNU_F04FHC7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "print('Music Perception (stimulus)')\n",
        "exp = ExP(1, 12, 64, 960)\n",
        "\n",
        "enc = False\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_per, y_train_per, X_test_per, y_test_per, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUUWaV2WG6KY",
        "outputId": "88d0d28d-d85d-4b18-ebbd-1d9a9b3394b5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Perception (stimulus)\n",
            "(432, 1, 64, 456)\n",
            "Epoch: 2000   Train loss: 0.010663   Test loss: 19.676662   Train accuracy 1.000000   Test accuracy is 0.083333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group (3) Classification"
      ],
      "metadata": {
        "id": "Zd7i1D3wwzNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Imagery"
      ],
      "metadata": {
        "id": "5LTvIoALHPru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "print('Music Imagery (group)')\n",
        "exp = ExP(1, 3, 64, 960)\n",
        "\n",
        "enc = False\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_img, group_y_train_img, X_test_img, group_y_test_img, enc)"
      ],
      "metadata": {
        "id": "HLxv7hx-A2qB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49b1a82-6a3e-42a6-dbb9-e552ab98cc50"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Imagery (group)\n",
            "(432, 1, 64, 456)\n",
            "Epoch: 2000   Train loss: 0.000446   Test loss: 15.840796   Train accuracy 1.000000   Test accuracy is 0.351852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Perception"
      ],
      "metadata": {
        "id": "dR1GztwaHZTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Perception (group)')\n",
        "exp = ExP(1, 3, 64, 960)\n",
        "\n",
        "enc = False\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_per, group_y_train_per, X_test_per, group_y_test_per, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjVDBUyLHd78",
        "outputId": "d8ebb760-b141-4bb4-d07b-db490fbf98ab"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Perception (group)\n",
            "(432, 1, 64, 456)\n",
            "Epoch: 2000   Train loss: 0.001320   Test loss: 13.617306   Train accuracy 1.000000   Test accuracy is 0.324074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meter (2) Classification"
      ],
      "metadata": {
        "id": "BnprMyqxw4lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Imagery"
      ],
      "metadata": {
        "id": "ASRCpKUBIwEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Imagery (meter)')\n",
        "exp = ExP(1, 2, 64, 960)\n",
        "\n",
        "enc = False\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_img, meter_y_train_img, X_test_img, meter_y_test_img, enc)"
      ],
      "metadata": {
        "id": "zM8Y-vHrDNnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21ee316-8eb8-469b-89e0-6115fd2c20a2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Imagery (meter)\n",
            "(432, 1, 64, 456)\n",
            "Epoch: 2000   Train loss: 0.000986   Test loss: 7.994064   Train accuracy 1.000000   Test accuracy is 0.574074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Perception"
      ],
      "metadata": {
        "id": "Cdp6wrJWIy8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Perception (meter)')\n",
        "exp = ExP(1, 2, 64, 960)\n",
        "\n",
        "enc = False\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_per, meter_y_train_per, X_test_per, meter_y_test_per, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZeowaULI1Ni",
        "outputId": "35ac3c6c-a8f9-47f4-bffc-6b7310a548fa"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Perception (meter)\n",
            "(432, 1, 64, 456)\n",
            "Epoch: 2000   Train loss: 0.000057   Test loss: 11.908955   Train accuracy 1.000000   Test accuracy is 0.481481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of the encoded hvEEGNet data"
      ],
      "metadata": {
        "id": "74B-UFbA-9Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Load and process the hvEEGNet encoded data"
      ],
      "metadata": {
        "id": "qIi5DFD7XLlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features for music imagery\n",
        "encoded_data_img = np.load(\"/content/c2_latent_features.npy\")\n",
        "X_train_img_enc, X_test_img_enc, y_train_img_enc, y_test_img_enc = train_test_split(encoded_data_img, y_encoded, test_size=0.2, random_state=42, stratify = y_encoded)\n",
        "X_train_img_enc = X_train_img_enc.transpose(0, 2, 1, 3)\n",
        "X_test_img_enc = X_test_img_enc.transpose(0, 2, 1, 3)\n",
        "# Features for music perception\n",
        "encoded_data_per = np.load(\"/content/c1_latent_features.npy\")\n",
        "X_train_per_enc , X_test_per_enc , y_train_per_enc , y_test_per_enc  = train_test_split(encoded_data_per, y_encoded, test_size=0.2, random_state=42, stratify = y_encoded)\n",
        "X_train_per_enc = X_train_per_enc.transpose(0, 2, 1, 3)\n",
        "X_test_per_enc = X_test_per_enc.transpose(0, 2, 1, 3)"
      ],
      "metadata": {
        "id": "1NcKYpgPNzio"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_img_enc.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDoPpCf7NmSl",
        "outputId": "d05adef7-58a3-4899-8c34-dce668d91e06"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(432, 1, 16, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels for the group condition\n",
        "group_y_train_img_enc = []\n",
        "group_y_test_img_enc = []\n",
        "for i in y_train_img_enc:\n",
        "    if i < 4:\n",
        "        group_y_train_img_enc.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_train_img_enc.append(1)\n",
        "    else:\n",
        "        group_y_train_img_enc.append(2)\n",
        "\n",
        "for i in y_test_img_enc:\n",
        "    if i < 4:\n",
        "        group_y_test_img_enc.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_test_img_enc.append(1)\n",
        "    else:\n",
        "        group_y_test_img_enc.append(2)\n",
        "\n",
        "group_y_train_per_enc = []\n",
        "group_y_test_per_enc = []\n",
        "for i in y_train_per_enc:\n",
        "    if i < 4:\n",
        "        group_y_train_per_enc.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_train_per_enc.append(1)\n",
        "    else:\n",
        "        group_y_train_per_enc.append(2)\n",
        "\n",
        "for i in y_test_per_enc:\n",
        "    if i < 4:\n",
        "        group_y_test_per_enc.append(0)\n",
        "    elif i < 8:\n",
        "        group_y_test_per_enc.append(1)\n",
        "    else:\n",
        "        group_y_test_per_enc.append(2)"
      ],
      "metadata": {
        "id": "MFlHwFj8M1wF"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels for the meter condition\n",
        "meter_y_train_img_enc = []\n",
        "meter_y_test_img_enc = []\n",
        "for i in y_train_img:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_train_img_enc.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_train_img_enc.append(1)\n",
        "\n",
        "for i in y_test_img_enc:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_test_img_enc.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_test_img_enc.append(1)\n",
        "\n",
        "meter_y_train_per_enc = []\n",
        "meter_y_test_per_enc = []\n",
        "for i in y_train_per:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_train_per_enc.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_train_per_enc.append(1)\n",
        "\n",
        "for i in y_test_per:\n",
        "  if i in [0, 1, 4, 5, 8, 9]:\n",
        "    meter_y_test_per_enc.append(0)\n",
        "  elif i in [2, 3, 6, 7, 10, 11]:\n",
        "    meter_y_test_per_enc.append(1)"
      ],
      "metadata": {
        "id": "1HvDvMI0bWOl"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conformer for the encoded data"
      ],
      "metadata": {
        "id": "sXqDlR9fWbEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use conv to capture local features, instead of postion embedding.\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, num_channels, emb_size=40):\n",
        "        # self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "\n",
        "        self.shallownet = nn.Sequential(\n",
        "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
        "            nn.Conv2d(40, 40, (num_channels, 1), (1, 1)),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 15), (1, 5)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.shallownet(x)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 num_heads=10,\n",
        "                 drop_p=0.5,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size, n_classes, size):\n",
        "        super().__init__()\n",
        "\n",
        "        # global average pooling\n",
        "        self.clshead = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        out = self.fc(x)\n",
        "        return x, out\n",
        "\n",
        "class Conformer(nn.Sequential):\n",
        "    def __init__(self, num_classes, num_channels, size, emb_size=40, depth=6):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(emb_size=emb_size, num_channels=num_channels),\n",
        "            TransformerEncoder(depth=depth, emb_size=emb_size),\n",
        "            ClassificationHead(emb_size, num_classes, size)\n",
        "        )\n",
        "\n",
        "\n",
        "class ExP():\n",
        "    def __init__(self, nsub, nclasses, nchannels, size):\n",
        "        super(ExP, self).__init__()\n",
        "        self.batch_size = 72\n",
        "        self.n_epochs = 2000\n",
        "        self.c_dim = 12\n",
        "        self.lr = 0.0002\n",
        "        self.b1 = 0.5\n",
        "        self.b2 = 0.999\n",
        "        self.dimension = (190, 50)\n",
        "        self.nSub = nsub\n",
        "        self.num_classes = nclasses\n",
        "        self.num_channels = nchannels\n",
        "        self.size = size\n",
        "\n",
        "        self.start_epoch = 0\n",
        "\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor\n",
        "        self.LongTensor = torch.cuda.LongTensor\n",
        "\n",
        "        self.criterion_l1 = torch.nn.L1Loss().cuda()\n",
        "        self.criterion_l2 = torch.nn.MSELoss().cuda()\n",
        "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        self.model = Conformer(self.num_classes, self.num_channels, self.size).cuda()\n",
        "        self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
        "        self.model = self.model.cuda()\n",
        "        # summary(self.model, (1, 64, 456))\n",
        "\n",
        "    def get_source_data(self, X_train, y_train, X_test, y_test, enc):\n",
        "        # ! please please recheck if you need validation set\n",
        "        # ! and the data segement compared methods used\n",
        "\n",
        "        # train data\n",
        "        self.train_data = X_train\n",
        "        self.train_label = y_train\n",
        "\n",
        "        if enc == False:\n",
        "          self.train_data = np.expand_dims(self.train_data, axis=1)\n",
        "        self.train_label = np.transpose(self.train_label)\n",
        "\n",
        "        self.allData = self.train_data\n",
        "        self.allLabel = self.train_label #[0]\n",
        "\n",
        "        shuffle_num = np.random.permutation(len(self.allData))\n",
        "        self.allData = self.allData[shuffle_num, :, :, :]\n",
        "        self.allLabel = self.allLabel[shuffle_num]\n",
        "\n",
        "        # test data\n",
        "        self.test_data = X_test\n",
        "        self.test_label = y_test\n",
        "\n",
        "        if enc == False:\n",
        "          self.test_data = np.expand_dims(self.test_data, axis=1)\n",
        "        self.test_label = np.transpose(self.test_label)\n",
        "\n",
        "        self.testData = self.test_data\n",
        "        self.testLabel = self.test_label#[0]\n",
        "\n",
        "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
        "        print(self.allData.shape)\n",
        "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, X_test, y_test, enc):\n",
        "\n",
        "        img, label, test_data, test_label = self.get_source_data(X_train, y_train, X_test, y_test, enc)\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        label = torch.from_numpy(label)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(img, label)\n",
        "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        test_data = torch.from_numpy(test_data)\n",
        "        test_label = torch.from_numpy(test_label)\n",
        "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "\n",
        "        test_data = Variable(test_data.type(self.Tensor))\n",
        "        test_label = Variable(test_label.type(self.LongTensor))\n",
        "\n",
        "        bestAcc = 0\n",
        "        averAcc = 0\n",
        "        num = 0\n",
        "        Y_true = 0\n",
        "        Y_pred = 0\n",
        "\n",
        "        # Train the cnn model\n",
        "        total_step = len(self.dataloader)\n",
        "        curr_lr = self.lr\n",
        "\n",
        "        for e in range(self.n_epochs):\n",
        "            # in_epoch = time.time()\n",
        "            self.model.train()\n",
        "            for i, (img, label) in enumerate(self.dataloader):\n",
        "\n",
        "                img = Variable(img.cuda().type(self.Tensor))\n",
        "                label = Variable(label.cuda().type(self.LongTensor))\n",
        "\n",
        "                tok, outputs = self.model(img)\n",
        "\n",
        "                loss = self.criterion_cls(outputs, label)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "\n",
        "            # out_epoch = time.time()\n",
        "\n",
        "\n",
        "            # test process\n",
        "            if (e + 1) % 1 == 0:\n",
        "                self.model.eval()\n",
        "                Tok, Cls = self.model(test_data)\n",
        "\n",
        "\n",
        "                loss_test = self.criterion_cls(Cls, test_label)\n",
        "                y_pred = torch.max(Cls, 1)[1]\n",
        "                acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
        "                train_pred = torch.max(outputs, 1)[1]\n",
        "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
        "                if e == 1999:\n",
        "                  print('Epoch:', e + 1,\n",
        "                        '  Train loss: %.6f' % loss.detach().cpu().numpy(),\n",
        "                        '  Test loss: %.6f' % loss_test.detach().cpu().numpy(),\n",
        "                        '  Train accuracy %.6f' % train_acc,\n",
        "                        '  Test accuracy is %.6f' % acc)\n",
        "\n",
        "                num = num + 1\n",
        "                averAcc = averAcc + acc\n",
        "                if acc > bestAcc:\n",
        "                    bestAcc = acc\n",
        "                    Y_true = test_label\n",
        "                    Y_pred = y_pred\n",
        "\n",
        "\n",
        "        averAcc = averAcc / num\n",
        "\n",
        "        return bestAcc, averAcc, Y_true, Y_pred\n",
        "        # writer.close()"
      ],
      "metadata": {
        "id": "RgiteafuWe5X"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stimulus (12) Classification with encoded data"
      ],
      "metadata": {
        "id": "94nZYzNoxC-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Imagery"
      ],
      "metadata": {
        "id": "DukgATicLaXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "print('Music Imagery (stimulus)')\n",
        "exp = ExP(1, 12, 16, 80)\n",
        "\n",
        "enc = True\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_img_enc, y_train_img_enc, X_test_img_enc, y_test_img_enc, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7bV5qdEMMJe",
        "outputId": "d3b2571f-fd82-4882-fc82-64093b7e1a3b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Imagery (stimulus)\n",
            "(432, 1, 16, 45)\n",
            "Epoch: 2000   Train loss: 0.031109   Test loss: 17.393209   Train accuracy 0.986111   Test accuracy is 0.157407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Perception"
      ],
      "metadata": {
        "id": "SOudbyldLcPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "print('Music Perception (stimulus)')\n",
        "exp = ExP(1, 12, 16, 80)\n",
        "\n",
        "enc = True\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_per_enc, y_train_per_enc, X_test_per_enc, y_test_per_enc, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYbseMDcMLyw",
        "outputId": "9fc60ea6-7a4b-4f40-df8c-abda1732de2a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Perception (stimulus)\n",
            "(432, 1, 16, 45)\n",
            "Epoch: 2000   Train loss: 0.058211   Test loss: 20.855639   Train accuracy 0.986111   Test accuracy is 0.074074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group (3) Classification with encoded data"
      ],
      "metadata": {
        "id": "S43WEvwfLU1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Imagery"
      ],
      "metadata": {
        "id": "dF7wEN72LegO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "print('Music Imagery (group)')\n",
        "exp = ExP(1, 3, 16, 80)\n",
        "\n",
        "enc = True\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_img_enc, group_y_train_img_enc, X_test_img_enc, group_y_test_img_enc, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybjWf4FRMLXz",
        "outputId": "8da5cacf-9db3-472a-d719-867f5f4950c3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Imagery (group)\n",
            "(432, 1, 16, 45)\n",
            "Epoch: 2000   Train loss: 0.000908   Test loss: 7.798090   Train accuracy 1.000000   Test accuracy is 0.416667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Perception"
      ],
      "metadata": {
        "id": "wF-vLwIWLgpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Perception (group)')\n",
        "exp = ExP(1, 3, 16, 80)\n",
        "\n",
        "enc = True\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_per_enc, group_y_train_per_enc, X_test_per_enc, group_y_test_per_enc, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mtWfj9IMK_K",
        "outputId": "1302eb1a-2cd1-4008-d6db-c6ea61694ba2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Perception (group)\n",
            "(432, 1, 16, 45)\n",
            "Epoch: 2000   Train loss: 0.000497   Test loss: 10.771924   Train accuracy 1.000000   Test accuracy is 0.333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meter (2) Classification with encoded data"
      ],
      "metadata": {
        "id": "_EAWNJQsLU9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Imagery"
      ],
      "metadata": {
        "id": "Y_ikFU4GMExd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Imagery (meter)')\n",
        "exp = ExP(1, 2, 16, 80)\n",
        "\n",
        "enc = True\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_img_enc, meter_y_train_img_enc, X_test_img_enc, meter_y_test_img_enc, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YGSvoezMKBE",
        "outputId": "763d9785-6edb-4c42-e826-9644655083b0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Imagery (meter)\n",
            "(432, 1, 16, 45)\n",
            "Epoch: 2000   Train loss: 0.000013   Test loss: 6.672926   Train accuracy 1.000000   Test accuracy is 0.537037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Music Perception"
      ],
      "metadata": {
        "id": "IkhFS2ryMHyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_n = 42\n",
        "print('seed is ' + str(seed_n))\n",
        "random.seed(seed_n)\n",
        "np.random.seed(seed_n)\n",
        "torch.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed(seed_n)\n",
        "torch.cuda.manual_seed_all(seed_n)\n",
        "\n",
        "\n",
        "print('Music Perception (meter)')\n",
        "exp = ExP(1, 2, 16, 80)\n",
        "\n",
        "enc = True\n",
        "bestAcc, averAcc, Y_true, Y_pred = exp.train(X_train_per_enc, meter_y_train_per_enc, X_test_per_enc, meter_y_test_per_enc, enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYyMJs_IMKZ4",
        "outputId": "3a3e6159-2efe-4d2c-b861-bd9758822668"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed is 42\n",
            "Music Perception (meter)\n",
            "(432, 1, 16, 45)\n",
            "Epoch: 2000   Train loss: 0.000722   Test loss: 5.976858   Train accuracy 1.000000   Test accuracy is 0.537037\n"
          ]
        }
      ]
    }
  ]
}